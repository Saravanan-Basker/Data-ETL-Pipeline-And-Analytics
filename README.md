ðŸ“˜ Project Overview

The YouTube Streaming ETL Pipeline is a real-time, production-ready data engineering project designed to continuously extract, transform, and load YouTube video and channel data using modern big data tools.

This Pipeline follows best practices in stream processing, fault tolerance, schema validation, and distributed analytics.


ðŸ› ï¸ Core Technologies
Tools | Role

ðŸ§  YouTube Data API | Data Source (Videos + Channels)
ðŸ›°ï¸ Apache Kafka | Real-time Streaming Layer
ðŸ”¥ Apache Spark PySpark | Stream Consumer + Transformation
ðŸ—ƒï¸ Apache Hadoop HDFS (Parquet) | Scalable Data Lake Storage
ðŸ§© Anaconda Jupyter Notebook | Data Analysis and finally create json file to store seen video id to prevent duplicates while excuting 
 

ðŸš€ Key Features

âœ… Initial Load: Fetches up to 50 videos per channel
ðŸ” Streaming Poll: Extracts new uploads every 10 minutes
ðŸ“¡ Kafka Integration: Seamless producer/consumer model
ðŸ§ª Spark Processing: Schema validation & transformation
ðŸ“ HDFS Parquet Output: Analytics-ready, columnar storage
ðŸš« Duplicate Prevention: Via seen_video_ids.json
ðŸ“Š Data Analytics Highlights

After successfully ingesting data into HDFS using the streaming ETL pipeline, we conducted exploratory data analysis using PySpark, Pandas, and Matplotlib in Jupyter Notebook (data_analysis.ipynb). The goal was to generate meaningful insights from real YouTube content and enhance visibility into media trends.
ðŸ” Key Analysis Techniques:

  ðŸ“Œ Top Video Analysis: Ranking videos by view count, likes, and comment activity.

    
  ðŸ’¬ Comment Insights:

   * Fetched top-level comments per video

   * Extracted common keywords using text preprocessing

  â˜ï¸ Word Cloud Generation:
   
   * Built word clouds from video titles, tags, and comments

   * Highlighted trending keywords and viewer themes

  ðŸ“ˆ Visualizations:

   * Bar charts, pie charts, and line plots to explore:

     * Most active channels

     * Engagement over time

     * Content format popularity

These analyses not only showcase the depth of data collected but also help inform strategies for content optimization, user engagement, and machine learning integration.




ðŸ”— Data Flow (Simplified Architecture)

YouTube API -> Kafka Producer (Python) -> Kafka Topic -> Kafka Consumer (PySpark) -> HDFS (Parquet)




ðŸ“Š Real-World Use Cases

  *  Track newly uploaded videos in near real-time

  *  Analyze comment engagement and sentiment

  *  Visualize accessibility insights (e.g., captioned content)

  *  Feed ML models with video metadata (quality, tags, etc.)











